{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TUGAS\n",
        "Prosedur pelatihan pada praktikum 2 merupakan prosedur sederhana, yang tidak memberi Anda banyak kendali. Model ini menggunakan \"teacher-forcing\" yang mencegah prediksi buruk diumpankan kembali ke model, sehingga model tidak pernah belajar untuk pulih dari kesalahan. Jadi, setelah Anda melihat cara menjalankan model secara manual, selanjutnya Anda akan mengimplementasikan custom loop pelatihan. Hal ini memberikan titik awal jika, misalnya, Anda ingin menerapkan pembelajaran kurikulum untuk membantu menstabilkan keluaran open-loop model. Bagian terpenting dari loop pelatihan khusus adalah fungsi langkah pelatihan.\n",
        "\n",
        "Gunakan untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca.\n",
        "\n",
        "Prosedurnya adalah:\n",
        "\n",
        "* Jalankan Model dan hitung loss dengan .\n",
        "* Hitung update dan terapkan pada model dengan optimizer"
      ],
      "metadata": {
        "id": "CKQFAg_BlvFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3BDqIyuskxOw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiFeTb5nmmGB",
        "outputId": "6572fa49-5996-4302-a1b5-35c791cc3e1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJTS97okrcVd",
        "outputId": "70feff0d-ddbd-43d9-f54b-07e14f1c0585"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8_35OSIrdVc",
        "outputId": "d67869d9-7d4d-40b2-a10a-69650960fd55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aPBvJfurfn0",
        "outputId": "141121d2-4269-4c04-88a9-248209981e67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique characters\n",
        "for char in vocab:\n",
        "    print(char, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcYU9PXYrhZ0",
        "outputId": "de5d24d1-b32e-4134-f4d9-247a75a42971"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTORIZE TEXT\n",
        "\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6UbAGg5roPY",
        "outputId": "13e16b1e-3ddc-43ab-8e15-3789cb92b06f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Membuat tf.keras.layers.StringLookup layer (Konvert Token jadi ID)\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "7PTBy1SGrw2w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9pvat6Br3bP",
        "outputId": "c5448ee2-03a7-4dcd-cd8d-1428677950f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengembalikan ID ke Token\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "KzbC4aFlsRA1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1ueobFrsSjA",
        "outputId": "f6213449-ecbf-4050-afaf-f9186ca7d441"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6COIZz78sbWj",
        "outputId": "e69df007-cb0b-4c93-e858-ea3593143d72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "znKoJMQMscvD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV7xDjPCsfAz",
        "outputId": "cda04922-8fa5-41fc-a9bf-e7f5214161ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "Ttmz-rrWsgBm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-cGER3Ysko9",
        "outputId": "70fa0bbc-f415-489f-c6d0-f46a0d6b6575"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "T9HTPIc2sl0h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKMmmXyDsmzz",
        "outputId": "1aa95cb6-7fbe-4318-d752-54208ba2d4b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat fungsi -> mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "KxGpYGF0svw0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdGWblwcs1s9",
        "outputId": "5fe102d1-ed83-42ca-c797-e35e3cc423e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "j1RojIs2s6aw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJwB1vwYs7uZ",
        "outputId": "c1901787-074a-42d2-af25-d27797eba7c5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCHING TRAINING\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMsSSNyKs9yb",
        "outputId": "d5919904-19d5-4b2c-e47a-da01ca54545c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat Model\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "wOB6upnttBY8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "jHZ9W4p9s_sh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "dm-bOisDtEhg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uji Model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26UFp-b8tFxM",
        "outputId": "5a62b16d-f06d-47fe-bb00-f94926d69a90"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq7VfwH0tHtu",
        "outputId": "6894bc79-d184-42db-93b5-a10d65c822b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "E-iODII_tKtH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJdrlVLftNTv",
        "outputId": "205035dc-2667-4909-84fe-da1f8145649d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([56, 58, 47, 12, 57,  7,  7, 34,  1,  9, 28, 20, 22, 52, 34, 13, 23,\n",
              "       60, 17, 61, 36, 65, 56,  7, 36, 59, 43, 29, 22, 34, 52, 30, 10, 14,\n",
              "       23, 12, 51, 28, 30, 35, 37, 45,  8, 51, 20, 26, 41, 36, 10, 42, 35,\n",
              "       18, 50, 59, 15, 13, 14, 50, 53, 43, 23, 32,  8, 20, 51, 31,  1, 60,\n",
              "       20, 18,  1, 12,  4, 21, 14, 39, 24,  7, 11, 23, 65, 42, 35, 34, 26,\n",
              "       41, 29, 48, 61, 29,  3, 30, 17, 56, 46, 14, 42,  7,  4, 45])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcfrpocGtO-4",
        "outputId": "7a4d834e-f511-482b-b2ff-f324ca596e8a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"When what's not meet, but what must be, was law,\\nThen were they chosen: in a better hour,\\nLet what i\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'qsh;r,,U\\n.OGImU?JuDvWzq,WtdPIUmQ3AJ;lOQVXf-lGMbW3cVEktB?AkndJS-GlR\\nuGE\\n;$HAZK,:JzcVUMbPivP!QDqgAc,$f'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "UlUJ0GF_tRSZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcCXaNw2tUe4",
        "outputId": "02ebbbf1-1b52-4f04-aa32-f907ef708bb8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.188841, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuzJd73MtWEK",
        "outputId": "e0a6fe62-5226-4ba9-8cfa-2aed947d96bc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.94631"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "74TykxRptYlB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "SHq1vJBZtZwQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb5OdwDJtb98",
        "outputId": "8193bfae-cc92-4c56-dce3-7ba7410badb7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 20s 58ms/step - loss: 2.7274\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.9960\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.7133\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.5499\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.4499\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3811\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.3291\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2842\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2430\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Teks\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "3n7cQfPMtgyY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "heMmRevZtliU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIW5wO7stnXn",
        "outputId": "22d566ad-48ec-4a35-dda3-7346305b22c9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Carest beg.\n",
            "But what, you will sleep in deny.\n",
            "\n",
            "BUCKINGHAM:\n",
            "There's some promise, and the matter give striting fathing;\n",
            "Which too riqudeapy, that my reputation be his\n",
            "lived in my dam, more than you hope to play ourselves! Soft!--\n",
            "A dight which I have not something agreed\n",
            "In banishment, till they ather means,\n",
            "Which is thine immortal toward, his heavy nature,\n",
            "Nagure, one sickening, and so head,\n",
            "For it be with such vill assised\n",
            "and bear to tell me together, unds, uninhory\n",
            "ThreelI thry you mean no opportes\n",
            "A nacking on a sea, ere he, then behold\n",
            "By yinged hands. What I come to-morrow,\n",
            "Would swim in quine own toward the pressment-ball: hath\n",
            "governantion that; I would be then, but yet my son was wretch\n",
            "That makest hor, it should bear, talk with nothing;\n",
            "And I can solely sort, to the people.\n",
            "\n",
            "Second Murderer:\n",
            "What but whose cour spirits him his kingdom sto mocking shame\n",
            "By Courts, by namers of breath,\n",
            "Ire you call drathless embraces, here, and a sisternoon,\n",
            "No both us Clarence I seem: whose w \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.280195474624634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrf-eh1rtquU",
        "outputId": "a986d8dc-69c3-40a9-f705-f98ffd9a0786"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nAll full your life,' so thrown the throne.\\n\\nDUKE OF YORK:\\nAs if he doubt, so the shape or you and beat quiet,\\nProofieg, sweet bastarry earth toge!\\nOf what his death is short not honour in great?\\nO, not so much, sir, and shall thou livest in ourselves?\\n\\nESCALUS:\\nMore!' belike my son, the park of Buckingham;\\nFor what are you mine,\\nThere shall pack not of him.\\n\\nCATESBY:\\nMy lord, by nothing speaks botlant,\\nFiend with blazes, winteful last issues!\\nSitifulance of quick, window. What ever you misthose soft worse?\\n\\nELBOW:\\nFaith, no more: something mad, some stopp'd, that's paesenge.\\nO, do thy flight--must needs to thum;\\nHis coudsels, his youth as you, my noble son,\\nThis sweet belay-like would say, will find my pardon.\\n\\nHASTINGS:\\nYou are by this provost; fellow only.\\n\\nCLIFFORD:\\nIf I have asparent to the threater,\\nEre I must, she will not, I hear,\\nThe sun hath strange queen, and so us down,\\nWith all three joints on my sword,\\nTrainous youth to privole, be it both you.\\n\\nCOMINIUS:\\nDoth the world s\"\n",
            " b\"ROMEO:\\nSucher! an oath ease: if\\nshe speak, good prayer and Took that dangers new,\\nBy occasion and own profincy,\\nSo black three peacets.\\n\\nMusician:\\nA bease loved, a misson truits, he supposes\\nA but a tody so frowns, how to bring your brothers\\nAnd make the cause of Menenius have done unto?\\n\\nISABELLA:\\nBy Bornee, carry there taste thee by juckets;;\\nI evented again, as I have sometaly,\\n'Tis crat her royal brave can Either fair,\\nOr I it liege, when I am ring'd on beautes;\\nFor I have thought, behind my mister.\\n\\nCETISTA:\\n\\nISABELLA:\\nOf speed:\\nWhat fear along?\\n\\nThird Citizen:\\nBerieve me, fond weful;\\nSo propper your agedy's rebulm. I think upon it.\\n\\nKING RICHARD III:\\nWe have still go wine! what love he twenty\\nA smallery gift, mistillester.\\n\\nSLY:\\nNine, cope, say you all be revengeal, and do as the heart.\\n\\nKING RICHARD III:\\nFor that with made the field, a very said,\\nWe are backled his woe, why shall be that birds\\nShe she had you can the commonwealth behind\\nWithin this double court horse with you;\\nFefter \"\n",
            " b\"ROMEO:\\nThe usurping how thou choler: if Edward, that\\nMurders them. This is dead of blasts;\\nThy warwickly, these graces brother,\\nAnd what to his environar us himself\\nTo petty out against us.\\n\\nMIRANDA:\\nNay, hearing, slighted nomicious,\\nMight have more stand, what to the breathest.\\n\\nPERDITA:\\nWhat streat me more?\\n\\nMessenger:\\nSir, if it be done as yet;\\n'tis the music and that doth preat his mother\\nHave stain'd with lie,\\nBe he the time\\nWhich estress in thousand time shorts, behind that dissembling\\nhands,\\nThat which madst thou that: all affood for your haste,\\nTurn roots\\nHis golden way I know no more:\\nMust deep the traitor in the come night of him,\\nIf thou remembers ours, inclined to Myssuler\\nand his ships\\nEndure in your cares, and whiles ut with\\nBecomested to thy beautholy antimphrorath.\\n\\nBRUTUS:\\nNor then, general, and that's not;\\nYour very hourless parture; then wish three specket! O, plead and world,\\nWith wholess the modestood prodent o' this faw and how to fine amazed.\\nWe know the tworn, as your\"\n",
            " b\"ROMEO:\\nNot up your blood, and\\nMy brave me noble, gose to the\\nOwer-song, and way, not and a mother's rage,\\n\\nToRIOLANUS:\\nWhat letters that dickle-times to my pretty Romeo, sir!\\n\\nGLOUCESTER:\\n\\nPage:\\nMidely, speak, men not, madam: he had ase,\\nBut that I ord, my lord; I heer that four mine,\\nWith Romeo sport to the state,\\nAnd therefore, mother, and that a flower yours.\\n\\nANTIGONUS:\\nWhat! boints bragsom with me?\\nO, sirrah; field, Kate done,\\nSo manyous terrors be valient attempt\\nTo grave and made in presence be't; the stones that did\\nWith gave him graceful death: look with an ain to a wife.\\n\\nKATHARINA:\\nMarry I, that, instead of this:\\nWhat is his lift; then unstales his victory,\\nFor the lists of her wife boldent: if I crave\\nUpon their abone: where you know not I know,\\nSo die, I would shame, A mestern beasts,\\nBy big, not know it, that, which we do 'twere to-day more than she\\nin shain my edge shall return'd with power.\\nUncle, I stay and shall a tooth. Come, marry The will!\\nOft thou a fear or Romeo, towar\"\n",
            " b\"ROMEO:\\nEmirables of you.\\n\\nCAMILLO:\\nBesides.\\n\\nLORD ROSS:\\nTell her him beat blegion, this is a more?\\n\\nPOLIXENES:\\nThou hadst made itis,\\nStill shortly, do not speak,\\nmany burren that with banishment have buenced to all echouses.\\n\\nVOLIONTA:\\nIs it as moaning.\\n\\nMERCUTIO:\\nThere's that he would not prove: you'll make him edemity\\nAnd break our daughterlands; and how he could find ours,\\nIf she had been the prince and speak to the time:\\nProvoked too, here as his honour's lands for't.\\n\\nCAPULET:\\nAy, sir?\\n'Al raish'd thou hast too moother;\\nTo criding a last in revolt conjured,\\nWhich 'twas the child, our prefared: no.\\n\\nNArlen:\\nHow now, uncoventy!\\nBy him, I am not with together,\\nCommon itaton'd, give me our hunting: I care,\\nWhat, have with instruction slight with it.\\n\\nDUKE VINCENTIO:\\nWe cat bid him hence doubles what you like a crame.\\nBefore you grow, sir, I am sitter. My Lord North masterhments;\\nBid when resisting only or she is;\\nThe house of What makes his eyes the world\\nOr shamefully moory, to the chaster\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.355980634689331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor Model Generator\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xape8ghts74",
        "outputId": "0d68ae53-3161-4b81-e713-e12ade7e82ec"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7c2d15b87160>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNWDVmx0tvhl",
        "outputId": "a5531ac6-76af-4068-9b6d-53a6d032551a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "O, if ere not at your ignorant father, good fello,\n",
            "With all storns that come, these thought you wit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JAWABAN\n",
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "uh51i3_Etyac"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "ku76C3ret1Vs"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "xMOWZxJJt3LY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Acjjeit4Tg",
        "outputId": "a826d195-3664-462d-ab52-8f03556ea662"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 56ms/step - loss: 2.7206\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c2ccb92fd90>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t9np1gdt5Z2",
        "outputId": "6bd3fe0f-43d5-4fbc-8a3d-083ee4999830"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1777\n",
            "Epoch 1 Batch 50 Loss 2.1126\n",
            "Epoch 1 Batch 100 Loss 1.9446\n",
            "Epoch 1 Batch 150 Loss 1.8546\n",
            "\n",
            "Epoch 1 Loss: 1.9902\n",
            "Time taken for 1 epoch 12.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8387\n",
            "Epoch 2 Batch 50 Loss 1.7133\n",
            "Epoch 2 Batch 100 Loss 1.6759\n",
            "Epoch 2 Batch 150 Loss 1.6603\n",
            "\n",
            "Epoch 2 Loss: 1.7091\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6016\n",
            "Epoch 3 Batch 50 Loss 1.5818\n",
            "Epoch 3 Batch 100 Loss 1.5705\n",
            "Epoch 3 Batch 150 Loss 1.5184\n",
            "\n",
            "Epoch 3 Loss: 1.5483\n",
            "Time taken for 1 epoch 11.65 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4506\n",
            "Epoch 4 Batch 50 Loss 1.4900\n",
            "Epoch 4 Batch 100 Loss 1.4687\n",
            "Epoch 4 Batch 150 Loss 1.4648\n",
            "\n",
            "Epoch 4 Loss: 1.4494\n",
            "Time taken for 1 epoch 12.05 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3894\n",
            "Epoch 5 Batch 50 Loss 1.3852\n",
            "Epoch 5 Batch 100 Loss 1.3830\n",
            "Epoch 5 Batch 150 Loss 1.3876\n",
            "\n",
            "Epoch 5 Loss: 1.3815\n",
            "Time taken for 1 epoch 11.98 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2953\n",
            "Epoch 6 Batch 50 Loss 1.2908\n",
            "Epoch 6 Batch 100 Loss 1.3638\n",
            "Epoch 6 Batch 150 Loss 1.3331\n",
            "\n",
            "Epoch 6 Loss: 1.3288\n",
            "Time taken for 1 epoch 11.42 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2544\n",
            "Epoch 7 Batch 50 Loss 1.2454\n",
            "Epoch 7 Batch 100 Loss 1.2988\n",
            "Epoch 7 Batch 150 Loss 1.3057\n",
            "\n",
            "Epoch 7 Loss: 1.2840\n",
            "Time taken for 1 epoch 10.86 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2538\n",
            "Epoch 8 Batch 50 Loss 1.2345\n",
            "Epoch 8 Batch 100 Loss 1.2159\n",
            "Epoch 8 Batch 150 Loss 1.2422\n",
            "\n",
            "Epoch 8 Loss: 1.2427\n",
            "Time taken for 1 epoch 10.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1946\n",
            "Epoch 9 Batch 50 Loss 1.2098\n",
            "Epoch 9 Batch 100 Loss 1.2236\n",
            "Epoch 9 Batch 150 Loss 1.2341\n",
            "\n",
            "Epoch 9 Loss: 1.2036\n",
            "Time taken for 1 epoch 10.83 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1116\n",
            "Epoch 10 Batch 50 Loss 1.1340\n",
            "Epoch 10 Batch 100 Loss 1.1526\n",
            "Epoch 10 Batch 150 Loss 1.1827\n",
            "\n",
            "Epoch 10 Loss: 1.1635\n",
            "Time taken for 1 epoch 10.99 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAWABAN SOAL 2\n",
        "Perbedaan mencolok antara kode tugas dan praktikum 2 terletak pada pendekatan pelatihan. Praktikum 2 memilih jalur yang lebih simpel dengan model.fit, sedangkan tugas menunjukkan pendekatan yang lebih kompleks. Dalam pendekatan ini, melibatkan definisi train_step dalam model turunan untuk mengatur pelatihan pada tingkat batch. Langkah-langkah eksplisit seperti perhitungan loss, gradien, dan pembaruan bobot model dengan apply_gradients digunakan, disertai dengan penggunaan objek tf.metrics.Mean untuk mengukur rata-rata loss selama pelatihan. Keseluruhan, pendekatan ini memberikan tingkat kontrol dan fleksibilitas yang lebih tinggi dalam mengelola pelatihan model."
      ],
      "metadata": {
        "id": "Em_XvqFXuWML"
      }
    }
  ]
}